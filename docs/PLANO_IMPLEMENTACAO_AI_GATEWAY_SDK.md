# Plano Detalhado: Implementa√ß√£o AI Gateway + AI SDK UI

**Data**: 2025-01-15  
**Objetivo**: Migrar sistema de chat para usar AI Gateway da Vercel e AI SDK UI

---

## üìã Vis√£o Geral

Este plano detalha a implementa√ß√£o completa de:
1. **AI Gateway da Vercel**: Unifica√ß√£o de endpoints, monitoramento e controle de custos
2. **AI SDK UI**: Streaming de respostas e c√≥digo mais limpo no frontend

**Benef√≠cios Esperados**:
- ‚úÖ Streaming de respostas em tempo real
- ‚úÖ Monitoramento de custos centralizado
- ‚úÖ C√≥digo 40-60% mais limpo
- ‚úÖ Persist√™ncia de conversas
- ‚úÖ Alta confiabilidade com fallback autom√°tico

**Tempo Estimado**: 8-12 horas

---

## üéØ Fase 1: Configura√ß√£o do AI Gateway

### 1.1. Criar AI Gateway no Dashboard da Vercel

**Tempo**: 15 minutos

**Passos**:

1. Acessar [Vercel Dashboard](https://vercel.com/dashboard)
2. Selecionar o projeto `villadelfiori` (ou criar se necess√°rio)
3. Navegar para **AI** ‚Üí **AI Gateway**
4. Clicar em **"Create Gateway"** ou **"Get Started"**
5. Configurar:
   - **Nome**: `gabi-ai-gateway` (ou similar)
   - **Provider Principal**: OpenAI
   - **Modelos**: 
     - `text-embedding-3-small` (embeddings)
     - `gpt-4o-mini` (chat)

### 1.2. Configurar Provedores

**Tempo**: 10 minutos

**Passos**:

1. No AI Gateway, adicionar provedor **OpenAI**:
   - **API Key**: Usar chave existente ou criar nova
   - **Modelos dispon√≠veis**: Selecionar modelos necess√°rios
   - **Configurar como provedor principal**

2. (Opcional) Adicionar provedores de fallback:
   - **Anthropic** (Claude)
   - **Google** (Gemini)
   - **Groq** (para velocidade)

### 1.3. Configurar Or√ßamentos e Alertas

**Tempo**: 10 minutos

**Passos**:

1. Configurar or√ßamentos por modelo:
   - **Embeddings** (`text-embedding-3-small`): $50/m√™s
   - **Chat** (`gpt-4o-mini`): $100/m√™s
   - **Total**: $150/m√™s

2. Configurar alertas:
   - Alerta em 80% do or√ßamento
   - Alerta em 95% do or√ßamento
   - Bloqueio autom√°tico em 100%

3. Configurar notifica√ß√µes:
   - Email para alertas
   - Webhook (opcional)

### 1.4. Obter Credenciais

**Tempo**: 5 minutos

**Passos**:

1. No AI Gateway, navegar para **Settings** ‚Üí **Authentication**
2. Copiar:
   - **Gateway URL**: Endpoint do gateway (ex: `https://gateway.vercel.ai/v1`)
   - **API Key**: Chave de autentica√ß√£o do gateway
3. Anotar credenciais em local seguro

**Resultado Esperado**:
- ‚úÖ AI Gateway configurado
- ‚úÖ Provedores configurados
- ‚úÖ Or√ßamentos e alertas configurados
- ‚úÖ Credenciais obtidas

---

## üéØ Fase 2: Instala√ß√£o de Depend√™ncias

### 2.1. Instalar AI SDK

**Tempo**: 5 minutos

**Comando**:
```bash
cd frontend
npm install ai @ai-sdk/openai
```

**Depend√™ncias Adicionadas**:
- `ai`: AI SDK Core e UI (~500KB)
- `@ai-sdk/openai`: Provider OpenAI para AI SDK

### 2.2. Verificar Vers√µes

**Tempo**: 2 minutos

**Verificar**:
- Node.js 20+ (j√° instalado)
- Next.js 14+ (j√° instalado)
- React 18+ (j√° instalado)

**Resultado Esperado**:
- ‚úÖ Depend√™ncias instaladas
- ‚úÖ Vers√µes compat√≠veis verificadas

---

## üéØ Fase 3: Criar API Route para Chat com Streaming

### 3.1. Criar Estrutura de Arquivos

**Tempo**: 5 minutos

**Arquivo**: `frontend/src/app/api/chat/route.ts`

### 3.2. Implementar API Route

**Tempo**: 2-3 horas

**Funcionalidades**:
1. Receber mensagens do frontend
2. Buscar contexto na base de conhecimento (Supabase)
3. Gerar embedding da pergunta (via AI Gateway)
4. Buscar chunks relevantes
5. Chamar LLM com streaming (via AI Gateway)
6. Retornar stream para o frontend

**C√≥digo Completo**:

```typescript
// frontend/src/app/api/chat/route.ts
import { openai } from '@ai-sdk/openai'
import { streamText } from 'ai'
import { createClient } from '@supabase/supabase-js'
import { NextRequest } from 'next/server'

// Configura√ß√£o do AI Gateway
const AI_GATEWAY_URL = process.env.VERCEL_AI_GATEWAY_URL || 'https://gateway.vercel.ai/v1'
const AI_GATEWAY_KEY = process.env.VERCEL_AI_GATEWAY_KEY

// Configura√ß√£o do Supabase
const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL!
const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY!

// Cliente Supabase com service role (para acesso completo)
// Usar service role para buscar na base de conhecimento
const supabaseAdmin = createClient(supabaseUrl, supabaseServiceKey)

// Modelos
const EMBEDDING_MODEL = 'text-embedding-3-small'
const CHAT_MODEL = 'gpt-4o-mini'
const EMBEDDING_DIMENSION = 1536

/**
 * Gera embedding usando AI Gateway
 */
async function generateEmbedding(text: string): Promise<number[]> {
  if (!AI_GATEWAY_KEY) {
    throw new Error('VERCEL_AI_GATEWAY_KEY n√£o configurada')
  }

  const response = await fetch(`${AI_GATEWAY_URL}/embeddings`, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${AI_GATEWAY_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: EMBEDDING_MODEL,
      input: text,
      dimensions: EMBEDDING_DIMENSION,
    }),
  })

  if (!response.ok) {
    const error = await response.text()
    throw new Error(`Erro ao gerar embedding: ${error}`)
  }

  const data = await response.json()
  return data.data[0].embedding
}

/**
 * Busca chunks relevantes na base de conhecimento
 */
async function searchKnowledgeBase(
  queryEmbedding: number[], 
  matchCount: number = 5,
  queryText?: string
) {
  // Tentar busca h√≠brida primeiro (se dispon√≠vel)
  if (queryText) {
    const { data: hybridData, error: hybridError } = await supabaseAdmin.rpc(
      'search_knowledge_base_hybrid',
      {
        query_embedding: queryEmbedding,
        query_text: queryText,
        match_threshold: 0.7,
        match_count: matchCount,
        filter_metadata: {},
      }
    ).catch(() => ({ data: null, error: { message: 'Function not found' } }))

    if (!hybridError && hybridData) {
      return hybridData
    }
  }

  // Fallback para busca vetorial simples
  const { data, error } = await supabaseAdmin.rpc('match_knowledge_base_documents', {
    query_embedding: queryEmbedding,
    match_threshold: 0.7,
    match_count: matchCount,
  })

  if (error) {
    console.error('Erro ao buscar na base de conhecimento:', error)
    return []
  }

  return data || []
}

export async function POST(req: NextRequest) {
  try {
    // Verificar autentica√ß√£o via header Authorization
    // O AI SDK UI pode passar o token do Supabase no header
    const authHeader = req.headers.get('authorization')
    
    // Se n√£o houver header, tentar validar via Supabase client
    // (opcional, dependendo da configura√ß√£o de seguran√ßa)
    
    // Parse do body (formato do AI SDK UI)
    const { messages } = await req.json()

    if (!messages || !Array.isArray(messages) || messages.length === 0) {
      return new Response('Mensagens s√£o obrigat√≥rias', { status: 400 })
    }

    const lastMessage = messages[messages.length - 1]
    if (lastMessage.role !== 'user') {
      return new Response('√öltima mensagem deve ser do usu√°rio', { status: 400 })
    }

    // Verificar configura√ß√£o do AI Gateway
    if (!AI_GATEWAY_KEY) {
      return new Response(
        JSON.stringify({ error: 'AI Gateway n√£o configurado' }),
        { status: 500, headers: { 'Content-Type': 'application/json' } }
      )
    }

    // 1. Gerar embedding da pergunta
    let queryEmbedding: number[]
    try {
      queryEmbedding = await generateEmbedding(lastMessage.content)
    } catch (error: any) {
      console.error('Erro ao gerar embedding:', error)
      return new Response(
        JSON.stringify({ error: 'Erro ao processar pergunta' }),
        { status: 500, headers: { 'Content-Type': 'application/json' } }
      )
    }

    // 2. Buscar chunks relevantes na base de conhecimento
    const relevantChunks = await searchKnowledgeBase(queryEmbedding, 5, lastMessage.content)

    // 3. Construir contexto
    let context = ''
    if (relevantChunks.length > 0) {
      context = relevantChunks
        .map((chunk: any) => {
          const source = chunk.metadata?.process_name || chunk.metadata?.document_title || 'Documento'
          return `[Fonte: ${source}]\n${chunk.content}`
        })
        .join('\n\n---\n\n')
    }

    // 4. Construir mensagens para o LLM
    const systemMessage = {
      role: 'system' as const,
      content: `Voc√™ √© a Gabi, S√≠ndica Virtual do Condom√≠nio Villa Dei Fiori. 
Voc√™ √© uma assistente inteligente que ajuda moradores, s√≠ndicos e administradores com informa√ß√µes sobre processos condominiais.

INSTRU√á√ïES:
- Seja sempre educada, profissional e prestativa
- Use APENAS as informa√ß√µes fornecidas no contexto para responder
- Se n√£o souber a resposta, diga que n√£o tem essa informa√ß√£o e sugira consultar a documenta√ß√£o
- Cite as fontes quando usar informa√ß√µes espec√≠ficas
- Responda em portugu√™s brasileiro
- Seja concisa mas completa

${context ? `\nCONTEXTO DISPON√çVEL:\n${context}` : '\nNenhum contexto espec√≠fico dispon√≠vel. Use seu conhecimento geral sobre gest√£o condominial.'}`,
    }

    // Preparar mensagens do hist√≥rico (√∫ltimas 10 para n√£o exceder contexto)
    // AI SDK UI j√° formata as mensagens corretamente
    const recentMessages = messages.slice(-10).map((msg: any) => ({
      role: msg.role as 'user' | 'assistant' | 'system',
      content: msg.content,
    }))

    // 5. Configurar OpenAI com AI Gateway
    // Usar AI Gateway URL como base URL customizada
    // Nota: AI SDK suporta baseURL customizada
    const openaiClient = openai({
      baseURL: AI_GATEWAY_URL,
      apiKey: AI_GATEWAY_KEY,
    })

    // 6. Stream resposta
    const result = await streamText({
      model: openaiClient(CHAT_MODEL),
      messages: [systemMessage, ...recentMessages],
      temperature: 0.7,
      maxTokens: 2000,
    })

    // 7. Retornar stream
    return result.toDataStreamResponse({
      headers: {
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Methods': 'POST, OPTIONS',
        'Access-Control-Allow-Headers': 'authorization, content-type',
      },
    })
  } catch (error: any) {
    console.error('Erro no endpoint de chat:', error)
    return new Response(
      JSON.stringify({ error: error.message || 'Erro ao processar chat' }),
      { 
        status: 500, 
        headers: { 'Content-Type': 'application/json' } 
      }
    )
  }
}

// Handler para OPTIONS (CORS)
export async function OPTIONS() {
  return new Response(null, {
    status: 200,
    headers: {
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'POST, OPTIONS',
      'Access-Control-Allow-Headers': 'authorization, content-type',
    },
  })
}
```

### 3.3. Configurar Vari√°veis de Ambiente

**Tempo**: 5 minutos

**Arquivo**: `.env.local` (desenvolvimento) e Vercel Dashboard (produ√ß√£o)

**Vari√°veis Necess√°rias**:
```env
# AI Gateway
VERCEL_AI_GATEWAY_URL=https://gateway.vercel.ai/v1
VERCEL_AI_GATEWAY_KEY=vgw_...

# Supabase (j√° existentes)
NEXT_PUBLIC_SUPABASE_URL=https://...
NEXT_PUBLIC_SUPABASE_ANON_KEY=...
SUPABASE_SERVICE_ROLE_KEY=...
```

**Configurar na Vercel**:
1. Dashboard ‚Üí Project ‚Üí Settings ‚Üí Environment Variables
2. Adicionar `VERCEL_AI_GATEWAY_URL`
3. Adicionar `VERCEL_AI_GATEWAY_KEY` (marcar como Sensitive)
4. Adicionar `SUPABASE_SERVICE_ROLE_KEY` (marcar como Sensitive)

**Resultado Esperado**:
- ‚úÖ API route criada
- ‚úÖ Integra√ß√£o com AI Gateway
- ‚úÖ Integra√ß√£o com base de conhecimento
- ‚úÖ Streaming funcionando

---

## üéØ Fase 4: Adaptar Edge Functions para AI Gateway

### 4.1. Atualizar `ingest-process`

**Tempo**: 1 hora

**Arquivo**: `supabase/functions/ingest-process/index.ts`

**Mudan√ßas**:
- Alterar endpoint de `https://api.openai.com/v1/embeddings` para AI Gateway
- Alterar autentica√ß√£o de `OPENAI_API_KEY` para `VERCEL_AI_GATEWAY_KEY`

**C√≥digo de Exemplo**:

```typescript
// Substituir
const OPENAI_API_KEY = Deno.env.get('OPENAI_API_KEY')
const OPENAI_API_URL = 'https://api.openai.com/v1/embeddings'

// Por
const AI_GATEWAY_URL = Deno.env.get('VERCEL_AI_GATEWAY_URL') || 'https://gateway.vercel.ai/v1'
const AI_GATEWAY_KEY = Deno.env.get('VERCEL_AI_GATEWAY_KEY')

// Na fun√ß√£o de gerar embedding
const response = await fetch(`${AI_GATEWAY_URL}/embeddings`, {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${AI_GATEWAY_KEY}`,
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: EMBEDDING_MODEL,
    input: text,
    dimensions: EMBEDDING_DIMENSION,
  }),
})
```

### 4.2. Atualizar `ingest-document`

**Tempo**: 1 hora

**Arquivo**: `supabase/functions/ingest-document/index.ts`

**Mudan√ßas**: Mesmas do `ingest-process`

**C√≥digo de Exemplo**:

```typescript
// ANTES (linha ~8)
const OPENAI_API_KEY = Deno.env.get("OPENAI_API_KEY")

// DEPOIS
const AI_GATEWAY_URL = Deno.env.get('VERCEL_AI_GATEWAY_URL') || 'https://gateway.vercel.ai/v1'
const AI_GATEWAY_KEY = Deno.env.get('VERCEL_AI_GATEWAY_KEY')

// Na fun√ß√£o de gerar embedding (linha ~98)
// ANTES
const response = await fetch("https://api.openai.com/v1/embeddings", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    "Authorization": `Bearer ${OPENAI_API_KEY}`,
  },
  // ...
})

// DEPOIS
if (!AI_GATEWAY_KEY) {
  throw new Error('VERCEL_AI_GATEWAY_KEY n√£o configurada')
}

const response = await fetch(`${AI_GATEWAY_URL}/embeddings`, {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    "Authorization": `Bearer ${AI_GATEWAY_KEY}`,
  },
  // ...
})
```

### 4.3. Atualizar Secrets no Supabase

**Tempo**: 5 minutos

**Passos**:
1. Supabase Dashboard ‚Üí Edge Functions ‚Üí Settings
2. Remover `OPENAI_API_KEY` (se existir)
3. Adicionar `VERCEL_AI_GATEWAY_URL`
4. Adicionar `VERCEL_AI_GATEWAY_KEY` (marcar como Secret)

**Resultado Esperado**:
- ‚úÖ Edge Functions atualizadas
- ‚úÖ Usando AI Gateway para embeddings
- ‚úÖ Secrets configurados

---

## üéØ Fase 5: Refatorar Frontend com AI SDK UI

### 5.1. Criar Hook Customizado (Opcional)

**Tempo**: 30 minutos

**Arquivo**: `frontend/src/lib/hooks/useChatWithRAG.ts`

**Funcionalidade**: Wrapper do `useChat` com l√≥gica espec√≠fica do projeto

```typescript
// frontend/src/lib/hooks/useChatWithRAG.ts
import { useChat } from 'ai/react'
import { useAuth } from '@/contexts/AuthContext'

export function useChatWithRAG() {
  const { user } = useAuth()

  const chat = useChat({
    api: '/api/chat',
    body: {
      conversationId: `conv-${user?.id || 'anonymous'}-${Date.now()}`,
      userId: user?.id,
    },
    onResponse: (response) => {
      // Callback opcional para processar resposta
      console.log('Resposta recebida:', response)
    },
    onError: (error) => {
      console.error('Erro no chat:', error)
    },
  })

  return chat
}
```

### 5.2. Refatorar Componente de Chat

**Tempo**: 2-3 horas

**Arquivo**: `frontend/src/app/(dashboard)/chat/page.tsx`

**C√≥digo Completo Refatorado**:

```typescript
"use client"

import { useEffect, useRef } from "react"
import { useRouter } from "next/navigation"
import { useAuth } from "@/contexts/AuthContext"
import { useRBAC } from "@/lib/hooks/useRBAC"
import { useChat } from 'ai/react'
import { Button } from "@/components/ui/button"
import { Input } from "@/components/ui/input"
import { Send, Loader2, Bot, User } from "lucide-react"
import ReactMarkdown from "react-markdown"
import { useChat } from 'ai/react'
import { supabase } from '@/lib/supabase/client'

export default function ChatPage() {
  const router = useRouter()
  const { isAuthenticated, isLoading: authLoading, user } = useAuth()
  const { canAccessChat } = useRBAC()
  const messagesEndRef = useRef<HTMLDivElement>(null)

  // Usar useChat do AI SDK UI
  const { messages, input, handleInputChange, handleSubmit, isLoading, error } = useChat({
    api: '/api/chat',
    body: {
      conversationId: `conv-${user?.id || 'anonymous'}-${Date.now()}`,
      userId: user?.id,
    },
    initialMessages: [
      {
        id: '1',
        role: 'assistant',
        content: 'Ol√°! Sou a Gabi, S√≠ndica Virtual do Condom√≠nio Villa Dei Fiori. Como posso ajud√°-lo hoje?',
      },
    ],
    onError: (error) => {
      console.error('Erro no chat:', error)
    },
  })

  // Redirecionar se n√£o pode acessar chat
  useEffect(() => {
    if (!authLoading && (!isAuthenticated || !canAccessChat())) {
      if (!isAuthenticated) {
        router.push("/login")
      } else if (!canAccessChat()) {
        router.push("/auth/unauthorized")
      }
    }
  }, [authLoading, isAuthenticated, canAccessChat, router])

  // Auto-scroll para √∫ltima mensagem
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" })
  }, [messages])

  if (authLoading || !isAuthenticated || !canAccessChat()) {
    return (
      <div className="h-[calc(100vh-73px)] flex items-center justify-center">
        <Loader2 className="h-8 w-8 animate-spin text-muted-foreground stroke-1" />
      </div>
    )
  }

  return (
    <div className="h-[calc(100vh-73px)] flex flex-col">
      {/* Header */}
      <div className="flex-shrink-0 border-b border-border/50 bg-card px-4 py-3">
        <div className="flex items-center gap-2">
          <div className="p-2 rounded-full bg-primary/10">
            <Bot className="h-5 w-5 text-primary stroke-1" />
          </div>
          <div>
            <h1 className="text-base font-medium text-foreground">Gabi</h1>
            <p className="text-xs text-muted-foreground">S√≠ndica Virtual</p>
          </div>
        </div>
      </div>

      {/* Messages Area */}
      <div className="flex-1 overflow-y-auto px-4 py-4 space-y-4">
        {messages.map((message) => (
          <div
            key={message.id}
            className={`flex gap-3 ${
              message.role === "user" ? "justify-end" : "justify-start"
            }`}
          >
            {message.role === "assistant" && (
              <div className="flex-shrink-0 w-8 h-8 rounded-full bg-primary/10 flex items-center justify-center">
                <Bot className="h-4 w-4 text-primary stroke-1" />
              </div>
            )}

            <div
              className={`max-w-[80%] rounded-2xl px-4 py-2 ${
                message.role === "user"
                  ? "bg-primary text-white"
                  : "bg-muted text-foreground"
              }`}
            >
              {message.role === "assistant" ? (
                <div className="prose prose-sm dark:prose-invert max-w-none">
                  <ReactMarkdown>{message.content}</ReactMarkdown>
                </div>
              ) : (
                <p className="text-sm whitespace-pre-wrap">{message.content}</p>
              )}
            </div>

            {message.role === "user" && (
              <div className="flex-shrink-0 w-8 h-8 rounded-full bg-muted flex items-center justify-center">
                <User className="h-4 w-4 text-foreground stroke-1" />
              </div>
            )}
          </div>
        ))}

        {isLoading && (
          <div className="flex gap-3 justify-start">
            <div className="flex-shrink-0 w-8 h-8 rounded-full bg-primary/10 flex items-center justify-center">
              <Bot className="h-4 w-4 text-primary stroke-1" />
            </div>
            <div className="bg-muted rounded-2xl px-4 py-2">
              <Loader2 className="h-4 w-4 animate-spin text-muted-foreground stroke-1" />
            </div>
          </div>
        )}

        {error && (
          <div className="flex gap-3 justify-start">
            <div className="bg-destructive/10 text-destructive rounded-2xl px-4 py-2">
              <p className="text-sm">Erro: {error.message}</p>
            </div>
          </div>
        )}

        <div ref={messagesEndRef} />
      </div>

      {/* Input Area */}
      <div className="flex-shrink-0 border-t border-border bg-card p-4">
        <form onSubmit={handleSubmit} className="flex gap-2">
          <Input
            value={input}
            onChange={handleInputChange}
            placeholder="Digite sua mensagem..."
            disabled={isLoading}
            className="flex-1 min-h-[44px]"
            autoFocus
          />
          <Button
            type="submit"
            disabled={!input.trim() || isLoading}
            className="min-w-[44px] min-h-[44px] bg-primary hover:bg-primary/90"
          >
            {isLoading ? (
              <Loader2 className="h-5 w-5 animate-spin stroke-1" />
            ) : (
              <Send className="h-5 w-5 stroke-1" />
            )}
          </Button>
        </form>
      </div>
    </div>
  )
}
```

### 5.3. Remover C√≥digo Antigo

**Tempo**: 15 minutos

**Arquivos a Limpar**:
- `frontend/src/lib/api/chat.ts` - Pode ser removido ou mantido para compatibilidade
- Remover imports n√£o utilizados

**Resultado Esperado**:
- ‚úÖ Componente refatorado
- ‚úÖ Streaming funcionando
- ‚úÖ C√≥digo mais limpo (~150 linhas vs ~260 linhas)

---

## üéØ Fase 6: Testes e Valida√ß√£o

### 6.1. Testes Locais

**Tempo**: 1-2 horas

**Checklist**:

1. **Testar API Route**:
   - [ ] Endpoint `/api/chat` responde corretamente
   - [ ] Streaming funciona (respostas aparecem em tempo real)
   - [ ] Busca na base de conhecimento funciona
   - [ ] Embeddings s√£o gerados corretamente
   - [ ] Erros s√£o tratados adequadamente

2. **Testar Frontend**:
   - [ ] Chat carrega corretamente
   - [ ] Mensagens s√£o exibidas
   - [ ] Streaming funciona (texto aparece progressivamente)
   - [ ] Auto-scroll funciona
   - [ ] Loading states funcionam
   - [ ] Erros s√£o exibidos

3. **Testar Edge Functions**:
   - [ ] `ingest-process` funciona com AI Gateway
   - [ ] `ingest-document` funciona com AI Gateway
   - [ ] Embeddings s√£o gerados corretamente

### 6.2. Testes de Integra√ß√£o

**Tempo**: 1 hora

**Cen√°rios**:
1. Enviar mensagem simples
2. Enviar mensagem que requer busca na base de conhecimento
3. Enviar m√∫ltiplas mensagens em sequ√™ncia
4. Testar com base de conhecimento vazia
5. Testar com erro de rede
6. Testar com AI Gateway indispon√≠vel (fallback)

### 6.3. Valida√ß√£o de Performance

**Tempo**: 30 minutos

**M√©tricas**:
- Tempo de primeira resposta (TTFR)
- Velocidade de streaming
- Uso de mem√≥ria
- Tamanho do bundle

**Resultado Esperado**:
- ‚úÖ Todos os testes passando
- ‚úÖ Streaming funcionando corretamente
- ‚úÖ Performance adequada

---

## üéØ Fase 7: Deploy e Monitoramento

### 7.1. Deploy do Frontend

**Tempo**: 15 minutos

**Passos**:
1. Commit de todas as mudan√ßas
2. Push para reposit√≥rio
3. Deploy autom√°tico na Vercel
4. Verificar build bem-sucedido

### 7.2. Deploy das Edge Functions

**Tempo**: 15 minutos

**Passos**:
1. Deploy de `ingest-process` atualizada
2. Deploy de `ingest-document` atualizada
3. Verificar logs de deploy

### 7.3. Configurar Monitoramento

**Tempo**: 30 minutos

**Passos**:
1. Verificar dashboard do AI Gateway:
   - Uso de tokens
   - Custo por modelo
   - Taxa de sucesso
   - Lat√™ncia

2. Configurar alertas:
   - Erros de API
   - Lat√™ncia alta
   - Uso excessivo

3. Verificar logs:
   - Logs da Vercel (API routes)
   - Logs do Supabase (Edge Functions)

**Resultado Esperado**:
- ‚úÖ Deploy conclu√≠do
- ‚úÖ Sistema funcionando em produ√ß√£o
- ‚úÖ Monitoramento configurado

---

## üéØ Fase 8: Documenta√ß√£o e Limpeza

### 8.1. Atualizar Documenta√ß√£o

**Tempo**: 30 minutos

**Arquivos a Atualizar**:
- `docs/CONFIGURAR_OPENAI_API_KEY.md` ‚Üí Atualizar para AI Gateway
- `docs/BASE_CONHECIMENTO.md` ‚Üí Mencionar AI Gateway
- `README.md` ‚Üí Atualizar instru√ß√µes de setup

### 8.2. Limpeza de C√≥digo

**Tempo**: 15 minutos

**A√ß√µes**:
- Remover c√≥digo comentado
- Remover imports n√£o utilizados
- Verificar linter errors
- Atualizar coment√°rios

**Resultado Esperado**:
- ‚úÖ Documenta√ß√£o atualizada
- ‚úÖ C√≥digo limpo

---

## üìä Cronograma Detalhado

| Fase | Descri√ß√£o | Tempo | Depend√™ncias |
|------|-----------|-------|--------------|
| **1** | Configura√ß√£o do AI Gateway | 40 min | - |
| **2** | Instala√ß√£o de Depend√™ncias | 7 min | Fase 1 |
| **3** | Criar API Route | 2-3h | Fase 1, 2 |
| **4** | Adaptar Edge Functions | 1h 10min | Fase 1 |
| **5** | Refatorar Frontend | 3h 15min | Fase 3 |
| **6** | Testes e Valida√ß√£o | 2h 30min | Fase 3, 4, 5 |
| **7** | Deploy e Monitoramento | 1h | Fase 6 |
| **8** | Documenta√ß√£o | 45 min | Fase 7 |
| **TOTAL** | | **8-12 horas** | |

---

## üîß Configura√ß√µes Necess√°rias

### Vari√°veis de Ambiente

#### Frontend (`.env.local` e Vercel)

```env
# AI Gateway
VERCEL_AI_GATEWAY_URL=https://gateway.vercel.ai/v1
VERCEL_AI_GATEWAY_KEY=vgw_...

# Supabase (j√° existentes)
NEXT_PUBLIC_SUPABASE_URL=https://...
NEXT_PUBLIC_SUPABASE_ANON_KEY=...
SUPABASE_SERVICE_ROLE_KEY=...
```

#### Supabase Edge Functions

```env
# AI Gateway
VERCEL_AI_GATEWAY_URL=https://gateway.vercel.ai/v1
VERCEL_AI_GATEWAY_KEY=vgw_...

# Supabase (j√° existentes)
SUPABASE_URL=https://...
SUPABASE_SERVICE_ROLE_KEY=...
```

---

## üß™ Checklist de Valida√ß√£o

### Pr√©-Implementa√ß√£o

- [ ] AI Gateway criado e configurado
- [ ] Credenciais obtidas
- [ ] Or√ßamentos configurados
- [ ] Depend√™ncias instaladas

### Durante Implementa√ß√£o

- [ ] API route criada e testada localmente
- [ ] Edge Functions atualizadas
- [ ] Frontend refatorado
- [ ] Streaming funcionando
- [ ] Busca na base de conhecimento funcionando

### P√≥s-Implementa√ß√£o

- [ ] Deploy conclu√≠do
- [ ] Testes em produ√ß√£o passando
- [ ] Monitoramento configurado
- [ ] Documenta√ß√£o atualizada
- [ ] C√≥digo limpo e revisado

---

## üö® Troubleshooting

### Problema: AI Gateway retorna 401

**Solu√ß√£o**:
- Verificar `VERCEL_AI_GATEWAY_KEY` est√° correta
- Verificar formato do header Authorization
- Verificar se a chave tem permiss√µes corretas

### Problema: Streaming n√£o funciona

**Solu√ß√£o**:
- Verificar se API route retorna `toDataStreamResponse()`
- Verificar se frontend usa `useChat` corretamente
- Verificar CORS headers

### Problema: Embeddings n√£o s√£o gerados

**Solu√ß√£o**:
- Verificar `VERCEL_AI_GATEWAY_KEY` nas Edge Functions
- Verificar logs das Edge Functions
- Verificar se modelo est√° dispon√≠vel no AI Gateway

### Problema: Base de conhecimento n√£o retorna resultados

**Solu√ß√£o**:
- Verificar se fun√ß√£o `match_knowledge_base_documents` existe
- Verificar se h√° documentos indexados
- Verificar threshold de similaridade

---

## üìà M√©tricas de Sucesso

### Antes da Implementa√ß√£o

- ‚ùå Sem streaming (respostas completas)
- ‚ùå ~260 linhas de c√≥digo no chat
- ‚ùå Sem monitoramento de custos
- ‚ùå Sem persist√™ncia de conversas

### Ap√≥s Implementa√ß√£o

- ‚úÖ Streaming funcionando
- ‚úÖ ~150 linhas de c√≥digo (redu√ß√£o de 40%)
- ‚úÖ Monitoramento de custos no AI Gateway
- ‚úÖ Persist√™ncia de conversas (se implementada)
- ‚úÖ C√≥digo mais limpo e manuten√≠vel

---

## üîó Refer√™ncias

- [AI Gateway Documentation](https://vercel.com/docs/ai-gateway)
- [AI SDK UI Documentation](https://ai-sdk.dev/docs/ai-sdk-ui/overview)
- [useChat Hook Reference](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat)
- [AI SDK Core Documentation](https://ai-sdk.dev/docs)

---

## üìù Notas Importantes

1. **Backup**: Fazer backup do c√≥digo atual antes de come√ßar
2. **Branch**: Criar branch separada para implementa√ß√£o
3. **Testes Incrementais**: Testar cada fase antes de prosseguir
4. **Rollback**: Manter c√≥digo antigo comentado inicialmente para rollback r√°pido
5. **Monitoramento**: Acompanhar m√©tricas do AI Gateway ap√≥s deploy

---

**√öltima Atualiza√ß√£o**: 2025-01-15

